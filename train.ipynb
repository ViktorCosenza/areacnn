{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "from functools import partial, reduce\n",
    "from tqdm import tqdm\n",
    "\n",
    "## Local Imports ##\n",
    "from models import helpers as model_helpers, models as custom_models\n",
    "from datasets import helpers as dataset_helpers, datasets as custom_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "W, H = (32, 32)\n",
    "BS = 128\n",
    "MAX_EPOCHS = 100\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DT_ROOT = 'data'\n",
    "POLYGON_COUNT_DIR = path.join(DT_ROOT, 'polygon_data_counts')\n",
    "POLYGON_PERCENTAGE_DIR = path.join(DT_ROOT, 'polygon_data_percentage')\n",
    "ELLIPSE_COUNT_DIR = path.join(DT_ROOT, 'ellipse_data_counts')\n",
    "ELLIPSE_PERCENTAGE_DIR = path.join(DT_ROOT, 'ellipse_data_percentage')\n",
    "TRANSFORM = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "dataset_generators = [\n",
    "    model_helpers.Param('RECT_COUNT', custom_datasets.get_dataset(\n",
    "        root_dir=POLYGON_COUNT_DIR, \n",
    "        df_path=path.join(POLYGON_COUNT_DIR, 'data.csv'),\n",
    "        transform=TRANSFORM,\n",
    "        bs=BS\n",
    "    )),\n",
    "    model_helpers.Param('RECT_PCT', custom_datasets.get_dataset(\n",
    "        root_dir=POLYGON_PERCENTAGE_DIR, \n",
    "        df_path=path.join(POLYGON_PERCENTAGE_DIR, 'data.csv'),\n",
    "        transform=TRANSFORM,\n",
    "        bs=BS\n",
    "    )),\n",
    "    model_helpers.Param('ELLIPSE_COUNT', custom_datasets.get_dataset(\n",
    "        root_dir=ELLIPSE_COUNT_DIR, \n",
    "        df_path=path.join(ELLIPSE_COUNT_DIR, 'data.csv'),\n",
    "        transform=TRANSFORM,\n",
    "        bs=BS\n",
    "    )),\n",
    "    model_helpers.Param('ELLIPSE_PCT', custom_datasets.get_dataset(\n",
    "        root_dir=ELLIPSE_PERCENTAGE_DIR, \n",
    "        df_path=path.join(ELLIPSE_PERCENTAGE_DIR, 'data.csv'),\n",
    "        transform=TRANSFORM,\n",
    "        bs=BS\n",
    "    ))\n",
    "]\n",
    "\n",
    "models_to_test = [\n",
    "    *custom_models.get_models(input_size=(1, W, H)), \n",
    "    model_helpers.Param('PERCEPTRON', lambda: nn.Sequential(model_helpers.Flatten(), nn.Linear(W * H, 1)))\n",
    "]\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will train 48 models\n",
      "6 MODELS\n",
      "4 DATASETS\n",
      "2 LOSS_FNS\n",
      "1 OPTIMS\n"
     ]
    }
   ],
   "source": [
    "OPTIMS = [\n",
    "    #model_helpers.Param('Adam', lambda: torch.optim.Adam),\n",
    "    model_helpers.Param('SGD', lambda: partial(torch.optim.SGD, lr=0.0005))\n",
    "]\n",
    "\n",
    "LOSS_FNS = [\n",
    "    model_helpers.Param('L1LOSS', lambda: model_helpers.squeeze_loss(nn.L1Loss())),\n",
    "    model_helpers.Param('MSELOSS', lambda: model_helpers.squeeze_loss(nn.MSELoss()))\n",
    "]\n",
    "\n",
    "\n",
    "grid = model_helpers.new_grid_search(models_to_test, OPTIMS, LOSS_FNS)\n",
    "grid = list(grid)\n",
    "\n",
    "print(f\"Will train {len(LOSS_FNS) * len(models_to_test) * len(dataset_generators) * len(OPTIMS)} models\")\n",
    "print(f\"{len(models_to_test)} MODELS\")\n",
    "print(f\"{len(dataset_generators)} DATASETS\")\n",
    "print(f\"{len(LOSS_FNS)} LOSS_FNS\")\n",
    "print(f\"{len(OPTIMS)} OPTIMS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:10<00:00,  2.53s/it]\n"
     ]
    }
   ],
   "source": [
    "## This is bad ##\n",
    "def normalize_train_test_df(df, indexes):\n",
    "    train_losses = (pd.DataFrame(df.train_losses.tolist(), index=df.set_index(indexes).index).stack()\n",
    "        .reset_index(name='train_losses'))\n",
    "    val_losses = (pd.DataFrame(df.val_losses.tolist(), index=df.set_index(indexes).index).stack()\n",
    "        .reset_index(name='val_losses'))\n",
    "    epoch = (pd.DataFrame(df.epoch.tolist(), index=df.set_index(indexes).index).stack()\n",
    "        .reset_index(name='epoch'))\n",
    "    val_losses[\"epoch\"] = epoch.epoch\n",
    "    val_losses[\"train_losses\"] = train_losses.train_losses\n",
    "    return val_losses.drop(columns=\"level_4\").set_index(indexes)\n",
    "\n",
    "\n",
    "columns = [\n",
    "    \"model_name\",\n",
    "    \"dataset\",\n",
    "    \"optim\",\n",
    "    \"loss_fn\",\n",
    "    \"train_loss\",\n",
    "    \"val_loss\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for dt in tqdm(dataset_generators):\n",
    "    dl_train = dt.param.train()\n",
    "    dl_test = dt.param.test()\n",
    "    for row in grid:\n",
    "        #print(f\"Pool: {row.model.name}\\nOpt: {row.opt.name}\\nLoss: {row.loss.name}\\nDT: {dt.name}\\n\")\n",
    "        metrics = model_helpers.train(\n",
    "            dl_train, \n",
    "            dl_test, \n",
    "            row.opt.param(),\n",
    "            row.loss.param(), \n",
    "            row.model.param(), \n",
    "            MAX_EPOCHS, \n",
    "            DEVICE)\n",
    "        rows = list(map(lambda r: {\n",
    "            \"epoch\": r[\"epoch\"],\n",
    "            \"train_loss\": r[\"train_loss\"],\n",
    "            \"val_loss\": r[\"val_loss\"],\n",
    "            \"model_name\": row.model.name,\n",
    "            \"optim\": row.opt.name,\n",
    "            \"loss_fn\": row.loss.name,\n",
    "            \"dataset\": dt.name}, metrics)) \n",
    "        df = df.append(rows, sort=True , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('FULL_RESULTS.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
