{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from random import randint\n",
    "\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from functools import partial, reduce\n",
    "from tqdm import tqdm\n",
    "\n",
    "import datetime\n",
    "\n",
    "## Local Imports ##\n",
    "from models import helpers as model_helpers, models as custom_models\n",
    "from datasets import helpers as dataset_helpers, datasets as custom_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, H = (512, 512)\n",
    "BS = 32\n",
    "MAX_EPOCHS = 100\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "DT_ROOT = 'data'\n",
    "POLYGON_COUNT_DIR = path.join(DT_ROOT, 'polygon_data_counts')\n",
    "POLYGON_PERCENTAGE_DIR = path.join(DT_ROOT, 'polygon_data_percentage')\n",
    "\n",
    "ELLIPSE_COUNT_DIR = path.join(DT_ROOT, 'ellipse_data_counts')\n",
    "ELLIPSE_PERCENTAGE_DIR = path.join(DT_ROOT, 'ellipse_data_percentage')\n",
    "\n",
    "VOC_SEGS_COUNTS_DIR = path.join('/home', 'victor', 'datasets', 'VOC_FORMS')\n",
    "\n",
    "TRANSFORM = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "print('Getting Datasets...')\n",
    "dataset_generators = [\n",
    "    model_helpers.Param('VOC_SEGS_COUNTS', custom_datasets.get_dataset(\n",
    "        root_dir=VOC_SEGS_COUNTS_DIR,\n",
    "        df_path=path.join(VOC_SEGS_COUNTS_DIR, 'data.csv'),\n",
    "        transform=TRANSFORM,\n",
    "        bs=BS\n",
    "    ))\n",
    "    #model_helpers.Param('RECT_COUNT', custom_datasets.get_dataset(\n",
    "    #    root_dir=POLYGON_COUNT_DIR, \n",
    "    #    df_path=path.join(POLYGON_COUNT_DIR, 'data.csv'),\n",
    "    #    transform=TRANSFORM,\n",
    "    #    bs=BS\n",
    "    #)),\n",
    "    #model_helpers.Param('RECT_PCT', custom_datasets.get_dataset(\n",
    "    #    root_dir=POLYGON_PERCENTAGE_DIR, \n",
    "    #    df_path=path.join(POLYGON_PERCENTAGE_DIR, 'data.csv'),\n",
    "    #    transform=TRANSFORM,\n",
    "    #    bs=BS\n",
    "    #)),\n",
    "    #model_helpers.Param('ELLIPSE_COUNT', custom_datasets.get_dataset(\n",
    "    #    root_dir=ELLIPSE_COUNT_DIR, \n",
    "    #    df_path=path.join(ELLIPSE_COUNT_DIR, 'data.csv'),\n",
    "    #    transform=TRANSFORM,\n",
    "    #    bs=BS\n",
    "    #)),\n",
    "    #model_helpers.Param('ELLIPSE_PCT', custom_datasets.get_dataset(\n",
    "    #    root_dir=ELLIPSE_PERCENTAGE_DIR, \n",
    "    #    df_path=path.join(ELLIPSE_PERCENTAGE_DIR, 'data.csv'),\n",
    "    #    transform=TRANSFORM,\n",
    "    #    bs=BS\n",
    "    #))\n",
    "]\n",
    "\n",
    "print('Getting Models...')\n",
    "models_to_test = [\n",
    "    *custom_models.get_models(input_size=(1, W, H)), \n",
    "    model_helpers.Param('PERCEPTRON', lambda: nn.Sequential(model_helpers.Flatten(), nn.Linear(W * H, 1)))\n",
    "]\n",
    "\n",
    "def assert_models(models, input_size):\n",
    "    for model in tqdm(models):\n",
    "        m = model.param().to(DEVICE)\n",
    "        try: m(torch.zeros(1, *input_size).to(DEVICE))\n",
    "        except Exception as e: print(f\"{model.name}: {e}\")\n",
    "\n",
    "## Avoid unwanted surprises ##\n",
    "print(\"Testing models...\")\n",
    "assert_models(models_to_test, (1, W, H))\n",
    "\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMS = [model_helpers.Param('Adam', lambda: torch.optim.Adam)]\n",
    "LOSS_FNS = [model_helpers.Param('L1LOSS', lambda: model_helpers.squeeze_loss(nn.L1Loss()))]\n",
    "\n",
    "\n",
    "grid = model_helpers.new_grid_search(models_to_test, OPTIMS, LOSS_FNS)\n",
    "grid = list(grid)\n",
    "\n",
    "print(f\"Will train {len(LOSS_FNS) * len(models_to_test) * len(dataset_generators) * len(OPTIMS)} models\")\n",
    "print(f\"{len(models_to_test)} MODELS\")\n",
    "print(f\"{len(dataset_generators)} DATASETS\")\n",
    "print(f\"{len(LOSS_FNS)} LOSS_FNS\")\n",
    "print(f\"{len(OPTIMS)} OPTIMS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"model_name\",\n",
    "    \"dataset\",\n",
    "    \"optim\",\n",
    "    \"loss_fn\",\n",
    "    \"train_loss\",\n",
    "    \"val_loss\"\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for dt in tqdm(dataset_generators):\n",
    "    dl_train = dt.param.train()\n",
    "    dl_test = dt.param.test()\n",
    "    for row in tqdm(grid):\n",
    "        metrics = model_helpers.train(\n",
    "            dl_train, \n",
    "            dl_test, \n",
    "            row.opt.param(),\n",
    "            row.loss.param(), \n",
    "            row.model.param(), \n",
    "            MAX_EPOCHS, \n",
    "            DEVICE\n",
    "        )\n",
    "        rows = list(map(lambda r: {\n",
    "            \"model_name\"     : row.model.name,\n",
    "            \"optim\"          : row.opt.name,\n",
    "            \"loss_fn\"        : row.loss.name,\n",
    "            \"dataset\"        : dt.name,\n",
    "            \"epoch\"          : r[\"epoch\"],\n",
    "            \"train_loss\"     : r[\"train_loss\"],\n",
    "            \"val_loss\"       : r[\"val_loss\"],\n",
    "            \"train_loss_avg\" : r[\"train_loss_avg\"],\n",
    "            \"val_loss_avg\"   : r[\"val_loss_avg\"]}, metrics)) \n",
    "        df = df.append(rows, sort=True , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(f'{str(datetime.datetime.now())}_FULL_RESULTS.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
